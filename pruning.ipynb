{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIhK9GwGqTLhTWEaGpWZpY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anil2k/Prune-Neural-Network/blob/main/pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define and initialize your neural network model.\n",
        "\n",
        "2. Record the weights of the model before pruning.\n",
        "\n",
        "3. Perform the pruning process on the model, setting some weights to zero.\n",
        "\n",
        "4. Fine-tune the pruned model if needed.\n",
        "\n",
        "5. Record the weights of the model after pruning.\n",
        "\n",
        "6. Calculate the weight changes by subtracting weights before pruning from weights after pruning.\n",
        "\n",
        "7. Analyze the weight changes to understand the impact of pruning on the model's weights.\n",
        "\n",
        "This process will help you observe how pruning has affected the weights in your neural network."
      ],
      "metadata": {
        "id": "9bSgK6R-kgt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary libraries, including PyTorch, to work with your neural network model."
      ],
      "metadata": {
        "id": "wlfT1e3jmI2m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "AG-ziGFVzVFU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils.prune as prune"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create the Neural Network Model:**\n",
        "\n",
        "Define your neural network model using PyTorch's nn.Module. For this example, let's assume you have a simple feedforward neural network."
      ],
      "metadata": {
        "id": "7RbApK7mmL8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(64, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oPseUDOezZhH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the Model:**\n",
        "\n",
        "Create an instance of your neural network and initialize it."
      ],
      "metadata": {
        "id": "2ZsFCEXnmaDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "id": "jNvARumKBuYW"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the weights before pruning\n",
        "print(\"Weights before pruning:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if 'weight' in name:  # Filter out weight tensors\n",
        "        print(f\"{name}: {param.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kAd155rhmbQ",
        "outputId": "cabd0f1c-c574-44ad-cdee-d38daeaab5d7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights before pruning:\n",
            "fc1.weight: tensor([[ 0.0193,  0.0884,  0.0630,  ..., -0.0782,  0.0537,  0.0424],\n",
            "        [ 0.0483, -0.1067,  0.0960,  ..., -0.0868, -0.0355,  0.0917],\n",
            "        [ 0.1136, -0.1091,  0.1016,  ...,  0.1048, -0.0786, -0.0734],\n",
            "        ...,\n",
            "        [ 0.0657, -0.0409,  0.1167,  ...,  0.1117, -0.0924,  0.0965],\n",
            "        [ 0.0360,  0.0062,  0.0316,  ..., -0.1010,  0.0425,  0.0933],\n",
            "        [ 0.0729, -0.0716,  0.1095,  ...,  0.0165,  0.0056, -0.0401]])\n",
            "fc2.weight: tensor([[ 0.0144,  0.0135, -0.0039,  ...,  0.0365,  0.0299, -0.0451],\n",
            "        [ 0.0714, -0.0416,  0.0592,  ..., -0.0413, -0.0011, -0.0872],\n",
            "        [ 0.0005, -0.0438,  0.0728,  ...,  0.0213, -0.0381, -0.0129],\n",
            "        ...,\n",
            "        [-0.0072,  0.0538, -0.0630,  ..., -0.0628, -0.0199,  0.0584],\n",
            "        [ 0.0583,  0.0012, -0.0753,  ...,  0.0472, -0.0534, -0.0863],\n",
            "        [ 0.0077, -0.0484,  0.0137,  ..., -0.0730,  0.0441,  0.0255]])\n",
            "fc3.weight: tensor([[-0.0846, -0.0899, -0.0101, -0.0182, -0.1071,  0.1141,  0.0585,  0.1218,\n",
            "         -0.0693, -0.0533, -0.0351, -0.0037, -0.0744,  0.1101,  0.0772,  0.0495,\n",
            "          0.0287, -0.0139,  0.0379, -0.0390, -0.0465, -0.0425,  0.0698, -0.0900,\n",
            "          0.0666,  0.0381, -0.1204,  0.0775,  0.0562,  0.0756,  0.0404,  0.0534,\n",
            "          0.0136,  0.0157,  0.0344,  0.0177,  0.0297,  0.0230,  0.1227,  0.0079,\n",
            "         -0.0834,  0.0742, -0.0338, -0.1217,  0.0578, -0.0557, -0.0360, -0.1101,\n",
            "          0.0951,  0.0763,  0.0517, -0.1232, -0.0395,  0.0617, -0.0499,  0.0171,\n",
            "         -0.1117, -0.0954, -0.0305, -0.0534,  0.1204,  0.0964,  0.0810, -0.0854],\n",
            "        [-0.0231, -0.0343,  0.0974,  0.0132,  0.0612,  0.0626,  0.0829, -0.0767,\n",
            "          0.0258, -0.0798,  0.1198,  0.0596, -0.1132,  0.1064,  0.1007, -0.0234,\n",
            "          0.0976, -0.0937, -0.0947, -0.0056,  0.0517,  0.1165, -0.0026,  0.0145,\n",
            "         -0.0140,  0.0245, -0.0278, -0.0688, -0.0874, -0.0455,  0.0234, -0.1150,\n",
            "          0.0887, -0.1230, -0.0433,  0.1199,  0.0457,  0.0170,  0.0668,  0.1148,\n",
            "          0.0185, -0.0383, -0.1140, -0.0205, -0.0903,  0.0819, -0.0711, -0.0231,\n",
            "          0.0086,  0.0224,  0.1132, -0.1125, -0.0274, -0.0629, -0.0828, -0.0889,\n",
            "         -0.0275, -0.1116, -0.0496,  0.1247, -0.0529,  0.0561,  0.0030, -0.1102],\n",
            "        [-0.1144, -0.0747,  0.0422,  0.0183, -0.0892, -0.0551,  0.0266, -0.0790,\n",
            "         -0.0905, -0.0888, -0.1194,  0.0108, -0.0012,  0.0642, -0.0924, -0.0722,\n",
            "         -0.0657, -0.0703, -0.0509,  0.0381,  0.0686, -0.0578, -0.0520,  0.0670,\n",
            "          0.0966,  0.0787, -0.0928, -0.0335, -0.0072, -0.1072,  0.0583,  0.0715,\n",
            "         -0.0966, -0.0711, -0.0938,  0.0859, -0.0215, -0.0939, -0.0576, -0.0327,\n",
            "          0.0614, -0.0743, -0.0462, -0.0919, -0.0971, -0.0425,  0.0019,  0.0758,\n",
            "          0.0094, -0.1153, -0.0619,  0.0809,  0.0226,  0.1024,  0.1155,  0.0756,\n",
            "          0.0920, -0.0098,  0.0791,  0.0304,  0.1238, -0.0779, -0.1185, -0.0718],\n",
            "        [ 0.1034, -0.0111, -0.1069, -0.0776,  0.0784,  0.1070, -0.0002,  0.0322,\n",
            "          0.0033,  0.1248, -0.0773, -0.0856, -0.1216, -0.0114,  0.0242,  0.1161,\n",
            "         -0.0903,  0.0243,  0.0951,  0.0800,  0.0846, -0.1004,  0.0167, -0.0765,\n",
            "         -0.0371,  0.0909,  0.1122, -0.0997,  0.0133,  0.1073, -0.0376, -0.0390,\n",
            "         -0.0037, -0.0630,  0.0908, -0.0196,  0.0801,  0.0931,  0.0922,  0.0340,\n",
            "          0.0864,  0.0770,  0.0145,  0.0415,  0.1134,  0.0521,  0.0806, -0.0839,\n",
            "          0.0559, -0.1171,  0.0610, -0.0422,  0.0905,  0.0896,  0.0169,  0.0420,\n",
            "          0.0740, -0.0176, -0.0523,  0.0654,  0.0201, -0.0189, -0.0037,  0.0986],\n",
            "        [-0.0926, -0.0566, -0.1249,  0.1192,  0.0494, -0.0588, -0.0507,  0.0849,\n",
            "         -0.0881, -0.0915,  0.1130,  0.0448, -0.0277,  0.1163,  0.0498, -0.0825,\n",
            "          0.0897, -0.0845,  0.0030,  0.0189,  0.0717, -0.1046,  0.0774,  0.0052,\n",
            "         -0.0675, -0.1042, -0.0445, -0.0651, -0.0083,  0.0409,  0.0959, -0.1057,\n",
            "         -0.0500, -0.0796, -0.0401, -0.0835, -0.0999, -0.1107, -0.0083,  0.1091,\n",
            "         -0.0589,  0.0548, -0.0674, -0.0707,  0.1206,  0.0090, -0.0827, -0.0389,\n",
            "         -0.0604, -0.0574,  0.0838,  0.1058,  0.0129,  0.0040,  0.0555,  0.0737,\n",
            "         -0.0438,  0.0688,  0.0994, -0.0834, -0.0580, -0.1046, -0.1158,  0.0198],\n",
            "        [ 0.0296, -0.0099,  0.0534, -0.0439,  0.0459,  0.0153,  0.1082,  0.0018,\n",
            "         -0.0985, -0.0368,  0.1081, -0.1172, -0.0325, -0.0235,  0.0298, -0.0692,\n",
            "          0.0880,  0.0112, -0.0215, -0.0535, -0.1230, -0.1002,  0.0631, -0.1215,\n",
            "          0.1155, -0.0984,  0.0464, -0.0019,  0.0846,  0.0531,  0.0395, -0.0827,\n",
            "         -0.0731, -0.0309, -0.0960,  0.0785,  0.0869, -0.0885,  0.1098, -0.0964,\n",
            "         -0.0765, -0.0676,  0.0033,  0.0241, -0.0894, -0.1127,  0.0017, -0.0274,\n",
            "         -0.0495,  0.1028,  0.0685, -0.0797,  0.0644,  0.0357,  0.0748, -0.0079,\n",
            "          0.0519,  0.0158,  0.0786, -0.0951, -0.0071, -0.0575,  0.1040, -0.0388],\n",
            "        [-0.1074, -0.0247,  0.0545, -0.0391, -0.0910,  0.0013,  0.0609,  0.0456,\n",
            "          0.0258,  0.0213,  0.0626, -0.1218,  0.0548, -0.0187, -0.0609, -0.0275,\n",
            "         -0.0844, -0.0007,  0.0878, -0.0018,  0.0628,  0.0269,  0.0089, -0.0627,\n",
            "         -0.0186,  0.0206,  0.0395,  0.1180, -0.0649,  0.0811,  0.0033,  0.0524,\n",
            "         -0.0466, -0.0014,  0.0938, -0.0848,  0.0207,  0.0517, -0.0791, -0.0786,\n",
            "         -0.0380, -0.0240, -0.0454,  0.0285,  0.0891, -0.0516, -0.0491, -0.0133,\n",
            "          0.0738, -0.0634, -0.1196,  0.1139,  0.1190,  0.0159,  0.1194,  0.0163,\n",
            "          0.0868, -0.0284, -0.1064, -0.0321,  0.0209, -0.0250,  0.0158, -0.0022],\n",
            "        [-0.0462,  0.1009, -0.0616,  0.1068,  0.1177,  0.1162,  0.0146,  0.0649,\n",
            "          0.0775,  0.0930, -0.0861, -0.0849,  0.0075, -0.0695, -0.1133,  0.0908,\n",
            "          0.0522,  0.0861, -0.0792,  0.1213, -0.0911, -0.0038, -0.0183,  0.0698,\n",
            "         -0.0497,  0.0492,  0.0798,  0.1142,  0.0906, -0.0566,  0.0020, -0.0686,\n",
            "          0.0970, -0.1093, -0.0273,  0.0972, -0.0756,  0.0715,  0.0513, -0.0184,\n",
            "          0.0547,  0.1116, -0.0508, -0.0826,  0.0909, -0.0952,  0.1006, -0.0805,\n",
            "          0.0341,  0.0356,  0.0324,  0.1004,  0.1233,  0.0251,  0.0549,  0.0960,\n",
            "          0.1168, -0.0217, -0.1060,  0.0770,  0.0295, -0.0063,  0.0274, -0.0112],\n",
            "        [-0.1214,  0.1212, -0.1193, -0.1056,  0.0787, -0.0888,  0.0414,  0.1219,\n",
            "          0.0804, -0.0045, -0.0339, -0.0076,  0.0860, -0.0775, -0.0803, -0.0063,\n",
            "          0.0778, -0.1083,  0.0461, -0.0712, -0.1149, -0.0952,  0.0214, -0.0431,\n",
            "         -0.1217,  0.0365, -0.0931, -0.0134, -0.0186,  0.0474,  0.1197,  0.0428,\n",
            "         -0.0145, -0.0591, -0.0950, -0.0126, -0.0434, -0.1174, -0.0262,  0.1121,\n",
            "         -0.0746,  0.1220,  0.0043, -0.0192,  0.0177,  0.0608, -0.1084,  0.0960,\n",
            "         -0.0614,  0.0299, -0.0374, -0.1132,  0.0067,  0.0072,  0.0636, -0.1080,\n",
            "          0.0928, -0.0864,  0.0975, -0.0131,  0.0343, -0.0460,  0.0329, -0.0177],\n",
            "        [ 0.0019,  0.0963, -0.0603, -0.0416,  0.0556, -0.1179, -0.0377,  0.0009,\n",
            "          0.0466,  0.0224, -0.0571, -0.1046, -0.0571,  0.0096, -0.0073,  0.0784,\n",
            "         -0.1066, -0.0590,  0.0166, -0.0604, -0.0003,  0.0900, -0.0769, -0.0777,\n",
            "         -0.0119,  0.1108,  0.0983, -0.0688, -0.0660, -0.0330, -0.0788, -0.0837,\n",
            "          0.1054,  0.0105, -0.1147, -0.0210, -0.0407, -0.1141, -0.0556,  0.1037,\n",
            "          0.0075, -0.0843, -0.0485,  0.0625, -0.0714,  0.1237,  0.0856,  0.0049,\n",
            "          0.0714, -0.0049, -0.0393,  0.0729, -0.0573,  0.0957,  0.1191,  0.0729,\n",
            "         -0.0551,  0.0838, -0.0860, -0.1134,  0.0658,  0.0859, -0.0039,  0.0147]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the weights before pruning\n",
        "weights_before_pruning = {name: param.data.clone() for name, param in model.named_parameters() if 'weight' in name}\n"
      ],
      "metadata": {
        "id": "cHOeS7d5i9Yx"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Pruning Method:**\n",
        "\n",
        "You can choose from various pruning methods provided by PyTorch, such as L1Unstructured, RandomUnstructured, or custom methods. In this example, we'll use L1Unstructured pruning."
      ],
      "metadata": {
        "id": "vBxfV5Rpmrz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_to_prune = (\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        ")\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2  # Specify the pruning rate (e.g., 20%)\n",
        ")"
      ],
      "metadata": {
        "id": "1bK8eS_xB4R9"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code prunes 20% of the weights in the specified layers (fc1 and fc2) using L1Unstructured pruning.\n",
        "\n",
        "Fine-Tune the Pruned Model:\n",
        "After pruning, it's a good practice to fine-tune the model to recover some of the lost performance. Train the model with your dataset as you normally would.\n",
        "\n",
        "Remove Pruning Masks (Optional):\n",
        "If you want to completely remove the pruning masks and make the model permanently pruned, you can do so using the following code:"
      ],
      "metadata": {
        "id": "SQkcY0HgnE0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for module, _ in parameters_to_prune:\n",
        "    prune.remove(module, 'weight')"
      ],
      "metadata": {
        "id": "HCmDA5SsCA9n"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the weights after pruning\n",
        "print(\"\\nWeights after pruning:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if 'weight' in name:  # Filter out weight tensors\n",
        "        print(f\"{name}: {param.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqMdlzhWiG3n",
        "outputId": "bb9b0b24-fdb6-49ee-dff3-7b1447f1289c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Weights after pruning:\n",
            "fc1.weight: tensor([[ 0.0000,  0.0884,  0.0630,  ..., -0.0782,  0.0537,  0.0424],\n",
            "        [ 0.0483, -0.1067,  0.0960,  ..., -0.0868, -0.0355,  0.0917],\n",
            "        [ 0.1136, -0.1091,  0.1016,  ...,  0.1048, -0.0786, -0.0734],\n",
            "        ...,\n",
            "        [ 0.0657, -0.0409,  0.1167,  ...,  0.1117, -0.0924,  0.0965],\n",
            "        [ 0.0360,  0.0000,  0.0316,  ..., -0.1010,  0.0425,  0.0933],\n",
            "        [ 0.0729, -0.0716,  0.1095,  ...,  0.0000,  0.0000, -0.0401]])\n",
            "fc2.weight: tensor([[ 0.0000,  0.0000, -0.0000,  ...,  0.0365,  0.0299, -0.0451],\n",
            "        [ 0.0714, -0.0416,  0.0592,  ..., -0.0413, -0.0000, -0.0872],\n",
            "        [ 0.0000, -0.0438,  0.0728,  ...,  0.0213, -0.0381, -0.0000],\n",
            "        ...,\n",
            "        [-0.0000,  0.0538, -0.0630,  ..., -0.0628, -0.0000,  0.0584],\n",
            "        [ 0.0583,  0.0000, -0.0753,  ...,  0.0472, -0.0534, -0.0863],\n",
            "        [ 0.0000, -0.0484,  0.0000,  ..., -0.0730,  0.0441,  0.0255]])\n",
            "fc3.weight: tensor([[-0.0846, -0.0899, -0.0101, -0.0182, -0.1071,  0.1141,  0.0585,  0.1218,\n",
            "         -0.0693, -0.0533, -0.0351, -0.0037, -0.0744,  0.1101,  0.0772,  0.0495,\n",
            "          0.0287, -0.0139,  0.0379, -0.0390, -0.0465, -0.0425,  0.0698, -0.0900,\n",
            "          0.0666,  0.0381, -0.1204,  0.0775,  0.0562,  0.0756,  0.0404,  0.0534,\n",
            "          0.0136,  0.0157,  0.0344,  0.0177,  0.0297,  0.0230,  0.1227,  0.0079,\n",
            "         -0.0834,  0.0742, -0.0338, -0.1217,  0.0578, -0.0557, -0.0360, -0.1101,\n",
            "          0.0951,  0.0763,  0.0517, -0.1232, -0.0395,  0.0617, -0.0499,  0.0171,\n",
            "         -0.1117, -0.0954, -0.0305, -0.0534,  0.1204,  0.0964,  0.0810, -0.0854],\n",
            "        [-0.0231, -0.0343,  0.0974,  0.0132,  0.0612,  0.0626,  0.0829, -0.0767,\n",
            "          0.0258, -0.0798,  0.1198,  0.0596, -0.1132,  0.1064,  0.1007, -0.0234,\n",
            "          0.0976, -0.0937, -0.0947, -0.0056,  0.0517,  0.1165, -0.0026,  0.0145,\n",
            "         -0.0140,  0.0245, -0.0278, -0.0688, -0.0874, -0.0455,  0.0234, -0.1150,\n",
            "          0.0887, -0.1230, -0.0433,  0.1199,  0.0457,  0.0170,  0.0668,  0.1148,\n",
            "          0.0185, -0.0383, -0.1140, -0.0205, -0.0903,  0.0819, -0.0711, -0.0231,\n",
            "          0.0086,  0.0224,  0.1132, -0.1125, -0.0274, -0.0629, -0.0828, -0.0889,\n",
            "         -0.0275, -0.1116, -0.0496,  0.1247, -0.0529,  0.0561,  0.0030, -0.1102],\n",
            "        [-0.1144, -0.0747,  0.0422,  0.0183, -0.0892, -0.0551,  0.0266, -0.0790,\n",
            "         -0.0905, -0.0888, -0.1194,  0.0108, -0.0012,  0.0642, -0.0924, -0.0722,\n",
            "         -0.0657, -0.0703, -0.0509,  0.0381,  0.0686, -0.0578, -0.0520,  0.0670,\n",
            "          0.0966,  0.0787, -0.0928, -0.0335, -0.0072, -0.1072,  0.0583,  0.0715,\n",
            "         -0.0966, -0.0711, -0.0938,  0.0859, -0.0215, -0.0939, -0.0576, -0.0327,\n",
            "          0.0614, -0.0743, -0.0462, -0.0919, -0.0971, -0.0425,  0.0019,  0.0758,\n",
            "          0.0094, -0.1153, -0.0619,  0.0809,  0.0226,  0.1024,  0.1155,  0.0756,\n",
            "          0.0920, -0.0098,  0.0791,  0.0304,  0.1238, -0.0779, -0.1185, -0.0718],\n",
            "        [ 0.1034, -0.0111, -0.1069, -0.0776,  0.0784,  0.1070, -0.0002,  0.0322,\n",
            "          0.0033,  0.1248, -0.0773, -0.0856, -0.1216, -0.0114,  0.0242,  0.1161,\n",
            "         -0.0903,  0.0243,  0.0951,  0.0800,  0.0846, -0.1004,  0.0167, -0.0765,\n",
            "         -0.0371,  0.0909,  0.1122, -0.0997,  0.0133,  0.1073, -0.0376, -0.0390,\n",
            "         -0.0037, -0.0630,  0.0908, -0.0196,  0.0801,  0.0931,  0.0922,  0.0340,\n",
            "          0.0864,  0.0770,  0.0145,  0.0415,  0.1134,  0.0521,  0.0806, -0.0839,\n",
            "          0.0559, -0.1171,  0.0610, -0.0422,  0.0905,  0.0896,  0.0169,  0.0420,\n",
            "          0.0740, -0.0176, -0.0523,  0.0654,  0.0201, -0.0189, -0.0037,  0.0986],\n",
            "        [-0.0926, -0.0566, -0.1249,  0.1192,  0.0494, -0.0588, -0.0507,  0.0849,\n",
            "         -0.0881, -0.0915,  0.1130,  0.0448, -0.0277,  0.1163,  0.0498, -0.0825,\n",
            "          0.0897, -0.0845,  0.0030,  0.0189,  0.0717, -0.1046,  0.0774,  0.0052,\n",
            "         -0.0675, -0.1042, -0.0445, -0.0651, -0.0083,  0.0409,  0.0959, -0.1057,\n",
            "         -0.0500, -0.0796, -0.0401, -0.0835, -0.0999, -0.1107, -0.0083,  0.1091,\n",
            "         -0.0589,  0.0548, -0.0674, -0.0707,  0.1206,  0.0090, -0.0827, -0.0389,\n",
            "         -0.0604, -0.0574,  0.0838,  0.1058,  0.0129,  0.0040,  0.0555,  0.0737,\n",
            "         -0.0438,  0.0688,  0.0994, -0.0834, -0.0580, -0.1046, -0.1158,  0.0198],\n",
            "        [ 0.0296, -0.0099,  0.0534, -0.0439,  0.0459,  0.0153,  0.1082,  0.0018,\n",
            "         -0.0985, -0.0368,  0.1081, -0.1172, -0.0325, -0.0235,  0.0298, -0.0692,\n",
            "          0.0880,  0.0112, -0.0215, -0.0535, -0.1230, -0.1002,  0.0631, -0.1215,\n",
            "          0.1155, -0.0984,  0.0464, -0.0019,  0.0846,  0.0531,  0.0395, -0.0827,\n",
            "         -0.0731, -0.0309, -0.0960,  0.0785,  0.0869, -0.0885,  0.1098, -0.0964,\n",
            "         -0.0765, -0.0676,  0.0033,  0.0241, -0.0894, -0.1127,  0.0017, -0.0274,\n",
            "         -0.0495,  0.1028,  0.0685, -0.0797,  0.0644,  0.0357,  0.0748, -0.0079,\n",
            "          0.0519,  0.0158,  0.0786, -0.0951, -0.0071, -0.0575,  0.1040, -0.0388],\n",
            "        [-0.1074, -0.0247,  0.0545, -0.0391, -0.0910,  0.0013,  0.0609,  0.0456,\n",
            "          0.0258,  0.0213,  0.0626, -0.1218,  0.0548, -0.0187, -0.0609, -0.0275,\n",
            "         -0.0844, -0.0007,  0.0878, -0.0018,  0.0628,  0.0269,  0.0089, -0.0627,\n",
            "         -0.0186,  0.0206,  0.0395,  0.1180, -0.0649,  0.0811,  0.0033,  0.0524,\n",
            "         -0.0466, -0.0014,  0.0938, -0.0848,  0.0207,  0.0517, -0.0791, -0.0786,\n",
            "         -0.0380, -0.0240, -0.0454,  0.0285,  0.0891, -0.0516, -0.0491, -0.0133,\n",
            "          0.0738, -0.0634, -0.1196,  0.1139,  0.1190,  0.0159,  0.1194,  0.0163,\n",
            "          0.0868, -0.0284, -0.1064, -0.0321,  0.0209, -0.0250,  0.0158, -0.0022],\n",
            "        [-0.0462,  0.1009, -0.0616,  0.1068,  0.1177,  0.1162,  0.0146,  0.0649,\n",
            "          0.0775,  0.0930, -0.0861, -0.0849,  0.0075, -0.0695, -0.1133,  0.0908,\n",
            "          0.0522,  0.0861, -0.0792,  0.1213, -0.0911, -0.0038, -0.0183,  0.0698,\n",
            "         -0.0497,  0.0492,  0.0798,  0.1142,  0.0906, -0.0566,  0.0020, -0.0686,\n",
            "          0.0970, -0.1093, -0.0273,  0.0972, -0.0756,  0.0715,  0.0513, -0.0184,\n",
            "          0.0547,  0.1116, -0.0508, -0.0826,  0.0909, -0.0952,  0.1006, -0.0805,\n",
            "          0.0341,  0.0356,  0.0324,  0.1004,  0.1233,  0.0251,  0.0549,  0.0960,\n",
            "          0.1168, -0.0217, -0.1060,  0.0770,  0.0295, -0.0063,  0.0274, -0.0112],\n",
            "        [-0.1214,  0.1212, -0.1193, -0.1056,  0.0787, -0.0888,  0.0414,  0.1219,\n",
            "          0.0804, -0.0045, -0.0339, -0.0076,  0.0860, -0.0775, -0.0803, -0.0063,\n",
            "          0.0778, -0.1083,  0.0461, -0.0712, -0.1149, -0.0952,  0.0214, -0.0431,\n",
            "         -0.1217,  0.0365, -0.0931, -0.0134, -0.0186,  0.0474,  0.1197,  0.0428,\n",
            "         -0.0145, -0.0591, -0.0950, -0.0126, -0.0434, -0.1174, -0.0262,  0.1121,\n",
            "         -0.0746,  0.1220,  0.0043, -0.0192,  0.0177,  0.0608, -0.1084,  0.0960,\n",
            "         -0.0614,  0.0299, -0.0374, -0.1132,  0.0067,  0.0072,  0.0636, -0.1080,\n",
            "          0.0928, -0.0864,  0.0975, -0.0131,  0.0343, -0.0460,  0.0329, -0.0177],\n",
            "        [ 0.0019,  0.0963, -0.0603, -0.0416,  0.0556, -0.1179, -0.0377,  0.0009,\n",
            "          0.0466,  0.0224, -0.0571, -0.1046, -0.0571,  0.0096, -0.0073,  0.0784,\n",
            "         -0.1066, -0.0590,  0.0166, -0.0604, -0.0003,  0.0900, -0.0769, -0.0777,\n",
            "         -0.0119,  0.1108,  0.0983, -0.0688, -0.0660, -0.0330, -0.0788, -0.0837,\n",
            "          0.1054,  0.0105, -0.1147, -0.0210, -0.0407, -0.1141, -0.0556,  0.1037,\n",
            "          0.0075, -0.0843, -0.0485,  0.0625, -0.0714,  0.1237,  0.0856,  0.0049,\n",
            "          0.0714, -0.0049, -0.0393,  0.0729, -0.0573,  0.0957,  0.1191,  0.0729,\n",
            "         -0.0551,  0.0838, -0.0860, -0.1134,  0.0658,  0.0859, -0.0039,  0.0147]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the weights after pruning\n",
        "weights_after_pruning = {name: param.data.clone() for name, param in model.named_parameters() if 'weight' in name}\n"
      ],
      "metadata": {
        "id": "HsEzKmsPjDJ9"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the weight changes\n",
        "print(\"Weight changes after pruning:\")\n",
        "for name in weights_before_pruning.keys():\n",
        "    weight_diff = weights_before_pruning[name] - weights_after_pruning[name]\n",
        "    print(f\"{name} change:\")\n",
        "    print(weight_diff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IA3Bz87jFG7",
        "outputId": "d77e21d3-8a50-412a-a882-ba0cc17f4ad9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight changes after pruning:\n",
            "fc1.weight change:\n",
            "tensor([[0.0193, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0056, 0.0000]])\n",
            "fc2.weight change:\n",
            "tensor([[ 0.0144,  0.0135, -0.0039,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0011,  0.0000],\n",
            "        [ 0.0005,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0129],\n",
            "        ...,\n",
            "        [-0.0072,  0.0000,  0.0000,  ...,  0.0000, -0.0199,  0.0000],\n",
            "        [ 0.0000,  0.0012,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0077,  0.0000,  0.0137,  ...,  0.0000,  0.0000,  0.0000]])\n",
            "fc3.weight change:\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the Pruned Model:**\n",
        "\n",
        "You can save the pruned model to a file for future use or deployment."
      ],
      "metadata": {
        "id": "9OjqsLQmnRVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'pruned_model.pth')\n"
      ],
      "metadata": {
        "id": "E03gf4M4BnbP"
      },
      "execution_count": 54,
      "outputs": []
    }
  ]
}